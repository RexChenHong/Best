import asyncio
import cupy as cp
import numpy as np
import torch
import pandas as pd
import datetime
import multiprocessing as mp
import glob
import logging
import queue
import gzip
import hashlib
import sqlite3
import uuid
from logging.handlers import TimedRotatingFileHandler, QueueHandler
from pathlib import Path
import chardet
import re
import psutil
import os
import time
from io import StringIO
from è¨­å®šæª” import è¨“ç·´è¨­å‚™, å¿«å–è³‡æ–™å¤¾, è¨“ç·´åƒæ•¸, è³‡æºé–¾å€¼, å¸‚å ´æ¸…å–®, é»å·®, é»å€¼
from å·¥å…·æ¨¡çµ„ import éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©, æª¢æŸ¥æª”æ¡ˆè·¯å¾‘, æ¸…ç†å¿«å–æª”æ¡ˆ, ç›£æ§ç¡¬é«”ç‹€æ…‹ä¸¦é™ç´š
from æ¨æ’­é€šçŸ¥æ¨¡çµ„ import ç™¼é€éŒ¯èª¤è¨Šæ¯, ç™¼é€é€šçŸ¥
from torch.utils.data import Subset, DataLoader
from sklearn.model_selection import train_test_split
import scipy.interpolate as interp

# åƒæ•¸é…ç½®
MAX_PROC = min(4, mp.cpu_count() - 1)
CHUNK_SIZE = 50000
MIN_CHUNK_SIZE = 10000
OVERLAP = 50
MAX_RAM_USE = 0.7 * psutil.virtual_memory().total
MAX_GPU_USE = 0.85
æŠ€è¡“æŒ‡æ¨™åå–® = ["SMA50", "HMA_16", "ATR_14", "VHF_28", "PivotHigh", "PivotLow"]
CACHE_VERSION = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

# æ—¥èªŒé…ç½®
log_dir = å¿«å–è³‡æ–™å¤¾.parent / "æ—¥èªŒ"
log_dir.mkdir(parents=True, exist_ok=True)
logger = logging.getLogger("è³‡æ–™é è™•ç†æ¨¡çµ„")
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s')
file_handler = TimedRotatingFileHandler(
    filename=log_dir / "preprocess_logs",
    when="midnight",
    backupCount=30,
    encoding='utf-8'
)
file_handler.setFormatter(formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
log_queue = queue.Queue(-1)
queue_handler = QueueHandler(log_queue)
logger.addHandler(queue_handler)

# GPU/CPUåˆ‡æ›è¨ˆæ•¸å™¨
gpu_switch_count = 0

def log_listener(queue, log_file_handler, log_console_handler):
    while True:
        try:
            record = queue.get()
            if record is None:
                break
            log_file_handler.handle(record)
            log_console_handler.handle(record)
        except Exception:
            break

def start_log_listener():
    listener = mp.Process(target=log_listener, args=(log_queue, file_handler, console_handler), daemon=True)
    listener.start()
    return listener

def calculate_md5(file_path):
    try:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"MD5è¨ˆç®—å¤±æ•— [{file_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E501ï¼šMD5è¨ˆç®—å¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        return None

def check_md5_consistency(source_path, cache_path):
    try:
        if not source_path.exists() or not cache_path.exists():
            logger.warning(f"MD5ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—ï¼šæª”æ¡ˆä¸å­˜åœ¨ [{source_path}, {cache_path}]")
            return False
        source_md5 = calculate_md5(source_path)
        cache_md5 = calculate_md5(cache_path)
        if source_md5 is None or cache_md5 is None:
            logger.error(f"MD5è¨ˆç®—ç„¡æ•ˆï¼š{source_path}, {cache_path}")
            return False
        is_consistent = source_md5 == cache_md5
        conn = sqlite3.connect(å¿«å–è³‡æ–™å¤¾.parent / "SQLite" / "é è™•ç†ç´€éŒ„.db")
        c = conn.cursor()
        c.execute("""
            CREATE TABLE IF NOT EXISTS é è™•ç†ç´€éŒ„è¡¨ (
                UUID TEXT PRIMARY KEY,
                å¸‚å ´ TEXT,
                é€±æœŸ TEXT,
                ä¾†æºæª”æ¡ˆå TEXT,
                ä¾†æºè·¯å¾‘ TEXT,
                ç­†æ•¸ INTEGER,
                æ¬„ä½å®Œæ•´æ€§ BOOLEAN,
                è™•ç†æ™‚é–“ REAL,
                GPUæ¨™è¨˜ BOOLEAN,
                ä¾†æºMD5 TEXT,
                å¿«å–MD5 TEXT,
                æª¢æŸ¥çµæœ BOOLEAN,
                å¿«å–æª”å TEXT,
                æ˜¯å¦æˆåŠŸ BOOLEAN,
                é‡è©¦æ¬¡æ•¸ INTEGER,
                å®Œæˆæ™‚é–“æˆ³ TEXT,
                ç¼ºå¤±ç‡ TEXT,
                ç•°å¸¸æ¯”ä¾‹ TEXT,
                ç‰ˆæœ¬è™Ÿ TEXT
            )
        """)
        uuid_val = str(uuid.uuid4())
        market, timeframe = source_path.stem.split("_")[:2]
        c.execute("INSERT INTO é è™•ç†ç´€éŒ„è¡¨ (UUID, å¸‚å ´, é€±æœŸ, ä¾†æºæª”æ¡ˆå, ä¾†æºè·¯å¾‘, æª¢æŸ¥çµæœ, ä¾†æºMD5, å¿«å–MD5, å®Œæˆæ™‚é–“æˆ³, ç‰ˆæœ¬è™Ÿ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                  (uuid_val, market, timeframe, source_path.name, str(source_path), is_consistent, source_md5, cache_md5, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), CACHE_VERSION))
        conn.commit()
        conn.close()
        if not is_consistent:
            logger.warning(f"MD5ä¸ä¸€è‡´ï¼š{source_path} vs {cache_path}")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E501ï¼šMD5ä¸ä¸€è‡´ [{source_path}]", market, timeframe, "è³‡æ–™é è™•ç†"))
        return is_consistent
    except Exception as e:
        logger.error(f"MD5ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•— [{source_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E501ï¼šMD5ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•— [{source_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E501ï¼šMD5ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—: {e}", "MD5æª¢æŸ¥éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return False

async def cleanup_expired_cache():
    try:
        cache_files = list(å¿«å–è³‡æ–™å¤¾.glob("*.npz"))
        current_time = datetime.datetime.now()
        for cache_file in cache_files:
            file_mtime = datetime.datetime.fromtimestamp(cache_file.stat().st_mtime)
            if (current_time - file_mtime).days > 7:
                cache_file.unlink()
                logger.info(f"æ¸…ç†éæœŸå¿«å–: {cache_file}")
                await ç™¼é€é€šçŸ¥(f"ã€é€šçŸ¥ã€‘æ¸…ç†éæœŸå¿«å–: {cache_file}", None, None, "è³‡æ–™é è™•ç†")
        return True
    except Exception as e:
        logger.error(f"å¿«å–æ¸…ç†å¤±æ•—: {e}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šå¿«å–æ¸…ç†å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†")
        await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šå¿«å–æ¸…ç†å¤±æ•—: {e}", "å¿«å–æ¸…ç†éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†")
        return False

def normalize_column_name(col):
    try:
        col = str(col).encode("utf-8").decode("utf-8-sig", errors="ignore")
        col = col.replace('\ufeff', '').replace('\r', '').replace('\n', '').replace('\t', '')
        col = col.replace('\u3000', '').replace(' ', '').replace('_', '').replace('-', '')
        col = col.lower()
        col = re.sub('[^a-z0-9\u4e00-\u9fff]', '', col)
        return col.strip()
    except Exception as e:
        logger.error(f"æ¬„ä½åç¨±æ¨™æº–åŒ–å¤±æ•—: {e}")
        return col

def smart_column_map(columns):
    mapping_dict = {
        "open": ["open", "é–‹", "é–‹ç›¤", "é–‹ç›¤åƒ¹", "é–‹å¸‚", "å§‹å€¤", "ã‚ªãƒ¼ãƒ—ãƒ³"],
        "high": ["high", "é«˜", "æœ€é«˜", "æœ€é«˜åƒ¹", "é«˜å€¤", "ãƒã‚¤"],
        "low": ["low", "ä½", "æœ€ä½", "æœ€ä½åƒ¹", "ä½å€¤", "ãƒ­ãƒ¼"],
        "close": ["close", "æ”¶", "æ”¶ç›¤", "æ”¶ç›¤åƒ¹", "çµæŸ", "çµ‚å€¤", "ã‚¯ãƒ­ãƒ¼ã‚¹"],
        "timestamp": ["timestamp", "datetime", "time", "date", "æ—¥æœŸ", "æ™‚é–“", "æˆäº¤æ™‚é–“", "æ™‚åˆ»", "æ™‚é–“æˆ³", "datetimeutc", "æ™‚é–“(utc)", "date(utc)"]
    }
    reverse_map = {normalize_column_name(v): std for std, variants in mapping_dict.items() for v in variants}
    return {col: reverse_map.get(normalize_column_name(col), col) for col in columns}

def guess_time_column(columns):
    candidates = ["timestamp", "datetime", "time", "date", "æ—¥æœŸ", "æ™‚é–“", "æˆäº¤æ™‚é–“", "æ™‚åˆ»", "æ™‚é–“æˆ³", "datetimeutc", "æ™‚é–“(utc)", "date(utc)"]
    normalized_cols = [normalize_column_name(col) for col in columns]
    for cand in candidates:
        if normalize_column_name(cand) in normalized_cols:
            return columns[normalized_cols.index(normalize_column_name(cand))]
    return None

def detect_encoding(filepath, sample_size=10000):
    try:
        with open(filepath, 'rb') as f:
            rawdata = f.read(sample_size)
        result = chardet.detect(rawdata)
        encoding = result['encoding'] if result and result['encoding'] else 'utf-8'
        if encoding not in ['utf-8', 'utf-8-sig', 'ascii', 'big5', 'gb2312']:
            logger.warning(f"æª¢æ¸¬åˆ°éæ¨™æº–ç·¨ç¢¼: {encoding}ï¼Œè½‰æ›ç‚º utf-8")
            encoding = 'utf-8'
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šéæ¨™æº–ç·¨ç¢¼ {encoding}ï¼Œè½‰æ›ç‚º utf-8 [{filepath}]", None, None, "è³‡æ–™é è™•ç†"))
        return encoding
    except Exception as e:
        logger.error(f"æª”æ¡ˆç·¨ç¢¼æª¢æ¸¬å¤±æ•— [{filepath}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæª”æ¡ˆç·¨ç¢¼æª¢æ¸¬å¤±æ•— [{filepath}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        return 'utf-8'

def load_zst_csv(file_path):
    try:
        import zstandard as zstd
        dctx = zstd.ZstdDecompressor()
        with open(file_path, 'rb') as f:
            stream_reader = dctx.stream_reader(f)
            text = stream_reader.read().decode('utf-8-sig', errors='replace')
            df = pd.read_csv(StringIO(text))
        required_cols = ['open', 'high', 'low', 'close', 'timestamp']
        col_map = smart_column_map(df.columns)
        df = df.rename(columns=col_map)
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols} [{file_path}]")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç¼ºå°‘å¿…è¦æ¬„ä½ {missing_cols} [{file_path}]", None, None, "è³‡æ–™é è™•ç†"))
            return None
        return df
    except Exception as e:
        logger.error(f"è®€å– Zstandard æª”æ¡ˆå¤±æ•— [{file_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šè®€å– Zstandard æª”æ¡ˆå¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šè®€å– Zstandard æª”æ¡ˆå¤±æ•—: {e}", "æª”æ¡ˆè®€å–éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†", str(file_path)))
        return None

def parse_datetime_column(df, col):
    try:
        df[col] = pd.to_datetime(df[col], infer_datetime_format=True, errors='coerce')
        return df[df[col].notnull()].sort_values(by=col).reset_index(drop=True)
    except Exception as e:
        logger.error(f"æ™‚é–“æ¬„ä½è§£æå¤±æ•— [{col}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ™‚é–“æ¬„ä½è§£æå¤±æ•— [{col}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæ™‚é–“æ¬„ä½è§£æå¤±æ•—: {e}", "æ™‚é–“è§£æéŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return df

def check_time_continuity(df, timeframe, file_path):
    try:
        if 'timestamp' not in df.columns:
            return True
        time_diffs = df['timestamp'].diff().dt.total_seconds().dropna()
        expected_interval = {"1m": 60, "5m": 300, "15m": 900, "30m": 1800, "1h": 3600, "4h": 14400, "1d": 86400}.get(timeframe, 900)
        gaps = time_diffs[time_diffs > expected_interval * 1.5]
        if len(gaps) > 0:
            logger.warning(f"æ™‚é–“åºåˆ—ä¸é€£çºŒ [{file_path}]ï¼Œæª¢æ¸¬åˆ° {len(gaps)} å€‹é–“éš™")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ™‚é–“åºåˆ—ä¸é€£çºŒï¼Œæª¢æ¸¬åˆ° {len(gaps)} å€‹é–“éš™ [{file_path}]", None, None, "è³‡æ–™é è™•ç†"))
            return False
        return True
    except Exception as e:
        logger.error(f"æ™‚é–“é€£çºŒæ€§æª¢æŸ¥å¤±æ•— [{file_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ™‚é–“é€£çºŒæ€§æª¢æŸ¥å¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        return False

def ensure_all_columns(df, required_cols):
    try:
        for col in required_cols:
            if col not in df.columns:
                df[col] = np.nan
        return df
    except Exception as e:
        logger.error(f"æ¬„ä½è£œå…¨å¤±æ•—: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ¬„ä½è£œå…¨å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†"))
        return df

def check_and_log_anomalies(df, log_writer, file_path):
    critical_cols = ['open', 'high', 'low', 'close', 'timestamp']
    problem = False
    missing_rates = {}
    anomaly_rates = {}
    market = Path(file_path).stem.split("_")[0]
    point_spread = é»å·®.get(market, é»å·®["default"])
    point_value = é»å€¼.get(market, é»å€¼["default"])
    for col in critical_cols:
        try:
            missing_rate = df[col].isna().mean()
            missing_rates[col] = missing_rate
            if col in df.columns and df[col].nunique() < 5:
                log_writer.write(f"â— æ¬„ä½[{col}] è®Šç•°åº¦æ¥µä½ï¼Œè³‡æ–™å¯èƒ½ç•°å¸¸ï¼š{file_path}\n")
                anomaly_rates[col] = 1.0
                asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ¬„ä½[{col}] è®Šç•°åº¦æ¥µä½ [{file_path}]", None, None, "è³‡æ–™é è™•ç†"))
                problem = True
            if col in ['open', 'high', 'low', 'close'] and col in df.columns:
                if (df[col] <= 0).mean() > 0.05:
                    log_writer.write(f"â— æ¬„ä½[{col}] éæ­£å€¼è¶…é5%ï¼š{file_path}\n")
                    anomaly_rates[col] = (df[col] <= 0).mean()
                    asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ¬„ä½[{col}] éæ­£å€¼è¶…é5% [{file_path}]", None, None, "è³‡æ–™é è™•ç†"))
                    problem = True
                if len(df[col]) > 100:
                    mean_price = df[col][:100].mean()
                    atr = df['ATR_14'][:100].mean() if 'ATR_14' in df.columns else 0.0
                    threshold = mean_price + max(3 * atr, point_spread * point_value * 5)
                    anomaly_rate = (df[col] > threshold).mean()
                    if anomaly_rate > 0:
                        log_writer.write(f"â— æ¬„ä½[{col}] ç•°å¸¸å€¼è¶…éå‡å€¼+max(3*ATR, 5*é»å·®*é»å€¼)ï¼š{file_path}\n")
                        anomaly_rates[col] = anomaly_rate
                        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ¬„ä½[{col}] ç•°å¸¸å€¼è¶…éå‡å€¼+max(3*ATR, 5*é»å·®*é»å€¼) [{file_path}]", None, None, "è³‡æ–™é è™•ç†"))
                        problem = True
        except Exception as e:
            logger.error(f"è³‡æ–™æª¢æŸ¥å¤±æ•— [{file_path}]: {e}")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šè³‡æ–™æª¢æŸ¥å¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
            problem = True
    if problem:
        log_writer.write(f"âŒ è³‡æ–™æª”æ¡ˆ[{file_path}] é—œéµæ¬„ä½ç•°å¸¸ï¼Œå»ºè­°æª¢æŸ¥ï¼\n")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šè³‡æ–™æª”æ¡ˆ[{file_path}] é—œéµæ¬„ä½ç•°å¸¸", None, None, "è³‡æ–™é è™•ç†"))
    conn = sqlite3.connect(å¿«å–è³‡æ–™å¤¾.parent / "SQLite" / "é è™•ç†ç´€éŒ„.db")
    c = conn.cursor()
    uuid_val = str(uuid.uuid4())
    market, timeframe = Path(file_path).stem.split("_")[:2]
    c.execute("UPDATE é è™•ç†ç´€éŒ„è¡¨ SET ç¼ºå¤±ç‡ = ?, ç•°å¸¸æ¯”ä¾‹ = ?, UUID = ? WHERE ä¾†æºæª”æ¡ˆå = ?",
              (str(missing_rates), str(anomaly_rates), uuid_val, Path(file_path).name))
    conn.commit()
    conn.close()
    asyncio.run(ç™¼é€é€šçŸ¥(f"ã€è³‡æ–™å®Œæ•´æ€§ã€‘æª”æ¡ˆ: {file_path}\nç¼ºå¤±ç‡: {missing_rates}\nç•°å¸¸æ¯”ä¾‹: {anomaly_rates}", market, timeframe, "è³‡æ–™é è™•ç†"))
    if problem:
        # è‡ªå‹•ä¿®å¾©ï¼šæ’å€¼ç•°å¸¸å€¼æˆ–æ¨æ£„
        for col in ['open', 'high', 'low', 'close']:
            if col in df.columns and anomaly_rates.get(col, 0) > 0:
                valid_mask = df[col].notna() & (df[col] > 0) & (df[col] <= threshold)
                if valid_mask.sum() > 0:
                    df[col] = df[col].interpolate(method='linear', limit_direction='both')
                    df[col] = np.where(df[col] > threshold, df[col].mean(), df[col])
                    log_writer.write(f"âœ… æ¬„ä½[{col}] ç•°å¸¸å€¼ä¿®å¾©å®Œæˆ\n")
    return not problem

def try_gpu_indicator(fn, *args, **kwargs):
    global gpu_switch_count
    try:
        if torch.cuda.is_available():
            gpu_prop = torch.cuda.get_device_properties(è¨“ç·´è¨­å‚™)
            gpu_max = int(gpu_prop.total_memory * MAX_GPU_USE)
            gpu_memory = torch.cuda.memory_allocated(è¨“ç·´è¨­å‚™)
            cpu_ok = psutil.cpu_percent() < è³‡æºé–¾å€¼["CPUä½¿ç”¨ç‡"] * 100
            if gpu_memory < gpu_max and cpu_ok:
                return fn(*args, **kwargs)
            else:
                gpu_switch_count += 1
                logger.warning(f"è³‡æºè¶…é™ï¼ˆGPUè¨˜æ†¶é«” {gpu_memory/1024/1024:.2f}MB/{gpu_max/1024/1024:.2f}MB æˆ– CPUä½¿ç”¨ç‡ï¼‰ï¼Œåˆ‡æ›è‡³CPUï¼Œç•¶å‰åˆ‡æ›æ¬¡æ•¸: {gpu_switch_count}")
                asyncio.run(ç™¼é€é€šçŸ¥(f"ã€é€šçŸ¥ã€‘è³‡æºè¶…é™ï¼ˆGPUè¨˜æ†¶é«” {gpu_memory/1024/1024:.2f}MB/{gpu_max/1024/1024:.2f}MB æˆ– CPUä½¿ç”¨ç‡ï¼‰ï¼Œåˆ‡æ›è‡³CPUï¼Œç•¶å‰åˆ‡æ›æ¬¡æ•¸: {gpu_switch_count}", None, None, "è³‡æ–™é è™•ç†"))
                torch.cuda.empty_cache()
        np_args = [cp.asnumpy(a) if isinstance(a, cp.ndarray) else a for a in args]
        return fn(*np_args, **kwargs)
    except Exception as e:
        gpu_switch_count += 1
        logger.error(f"æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å¤±æ•—ï¼Œåˆ‡æ›è‡³CPUï¼Œç•¶å‰åˆ‡æ›æ¬¡æ•¸: {gpu_switch_count}: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæŠ€è¡“æŒ‡æ¨™è¨ˆç®—å¤±æ•—ï¼Œåˆ‡æ›è‡³CPU: {e}", None, None, "è³‡æ–™é è™•ç†"))
        np_args = [cp.asnumpy(a) if isinstance(a, cp.ndarray) else a for a in args]
        return fn(*np_args, **kwargs)

def calc_sma(arr, window):
    try:
        if isinstance(arr, cp.ndarray):
            kernel = cp.ones(window, dtype=cp.float32) / window
            sma = cp.convolve(arr, kernel, mode='valid')
            nanpad = cp.full(window-1, cp.nan, dtype=cp.float32)
            return cp.concatenate([nanpad, sma])
        else:
            sma = np.convolve(arr, np.ones(window, dtype=np.float32) / window, mode='valid')
            nanpad = np.full(window-1, np.nan, dtype=np.float32)
            return np.concatenate([nanpad, sma])
    except Exception as e:
        logger.error(f"SMAè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"SMAè¨ˆç®—éŒ¯èª¤: {e}")

def calc_hma(arr, window):
    try:
        half = int(window / 2)
        sqrtw = int(np.sqrt(window))
        wma1 = calc_wma(arr, half)
        wma2 = calc_wma(arr, window)
        raw = 2 * wma1 - wma2
        return calc_wma(raw, sqrtw)
    except Exception as e:
        logger.error(f"HMAè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"HMAè¨ˆç®—éŒ¯èª¤: {e}")

def calc_wma(arr, window):
    try:
        if isinstance(arr, cp.ndarray):
            weights = cp.arange(1, window+1)
            wma = cp.full_like(arr, cp.nan, dtype=cp.float32)
            for i in range(window-1, len(arr)):
                wma[i] = cp.dot(arr[i-window+1:i+1], weights) / weights.sum()
            return wma
        else:
            weights = np.arange(1, window+1)
            wma = np.full_like(arr, np.nan, dtype=np.float32)
            for i in range(window-1, len(arr)):
                wma[i] = np.dot(arr[i-window+1:i+1], weights) / weights.sum()
            return wma
    except Exception as e:
        logger.error(f"WMAè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"WMAè¨ˆç®—éŒ¯èª¤: {e}")

def calc_atr(high, low, close, window):
    try:
        if isinstance(high, cp.ndarray):
            prev_close = cp.concatenate([cp.array([close[0]]), close[:-1]])
            tr = cp.maximum(high - low, cp.maximum(cp.abs(high - prev_close), cp.abs(low - prev_close)))
            return calc_sma(tr, window)
        else:
            prev_close = np.concatenate([np.array([close[0]]), close[:-1]])
            tr = np.maximum(high - low, np.maximum(np.abs(high - prev_close), np.abs(low - prev_close)))
            return calc_sma(tr, window)
    except Exception as e:
        logger.error(f"ATRè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"ATRè¨ˆç®—éŒ¯èª¤: {e}")

def calc_vhf(close, window):
    try:
        if isinstance(close, cp.ndarray):
            vhf = cp.full_like(close, cp.nan, dtype=cp.float32)
            for i in range(window-1, len(close)):
                maxp = cp.max(close[i-window+1:i+1])
                minp = cp.min(close[i-window+1:i+1])
                denominator = cp.sum(cp.abs(close[i-window+2:i+1] - close[i-window+1:i]))
                vhf[i] = (maxp - minp) / denominator if denominator != 0 else 0
            return vhf
        else:
            vhf = np.full_like(close, np.nan, dtype=np.float32)
            for i in range(window-1, len(close)):
                maxp = np.max(close[i-window+1:i+1])
                minp = np.min(close[i-window+1:i+1])
                denominator = np.sum(np.abs(close[i-window+2:i+1] - close[i-window+1:i]))
                vhf[i] = (maxp - minp) / denominator if denominator != 0 else 0
            return vhf
    except Exception as e:
        logger.error(f"VHFè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"VHFè¨ˆç®—éŒ¯èª¤: {e}")

def calc_pivot_high(arr, window):
    try:
        if isinstance(arr, cp.ndarray):
            pivot = cp.full_like(arr, cp.nan, dtype=cp.float32)
            for i in range(window, len(arr)-window):
                center = arr[i]
                if cp.all(center >= arr[i-window:i+window+1]):
                    pivot[i] = center
            return pivot
        else:
            pivot = np.full_like(arr, np.nan, dtype=np.float32)
            for i in range(window, len(arr)-window):
                center = arr[i]
                if np.all(center >= arr[i-window:i+window+1]):
                    pivot[i] = center
            return pivot
    except Exception as e:
        logger.error(f"PivotHighè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"PivotHighè¨ˆç®—éŒ¯èª¤: {e}")

def calc_pivot_low(arr, window):
    try:
        if isinstance(arr, cp.ndarray):
            pivot = cp.full_like(arr, cp.nan, dtype=cp.float32)
            for i in range(window, len(arr)-window):
                center = arr[i]
                if cp.all(center <= arr[i-window:i+window+1]):
                    pivot[i] = center
            return pivot
        else:
            pivot = np.full_like(arr, np.nan, dtype=np.float32)
            for i in range(window, len(arr)-window):
                center = arr[i]
                if np.all(center <= arr[i-window:i+window+1]):
                    pivot[i] = center
            return pivot
    except Exception as e:
        logger.error(f"PivotLowè¨ˆç®—éŒ¯èª¤: {e}")
        raise RuntimeError(f"PivotLowè¨ˆç®—éŒ¯èª¤: {e}")

def torch_tensorize(df, device=è¨“ç·´è¨­å‚™):
    try:
        numeric_df = df.select_dtypes(include=[np.number])
        if len(numeric_df) == 0:
            logger.error("ç„¡æ•¸å€¼æ¬„ä½å¯è½‰æ›ç‚º tensor")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯("éŒ¯èª¤ç¢¼E502ï¼šç„¡æ•¸å€¼æ¬„ä½å¯è½‰æ›ç‚º tensor", None, None, "è³‡æ–™é è™•ç†"))
            return None
        arr = cp.asarray(numeric_df.values, dtype=cp.float32)
        tensor = torch.as_tensor(arr, device=device)
        return tensor
    except Exception as e:
        logger.error(f"tensor è½‰æ›éŒ¯èª¤: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼štensor è½‰æ›éŒ¯èª¤: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼štensor è½‰æ›éŒ¯èª¤: {e}", "tensor è½‰æ›éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return None

def save_cache(file_path, tensor, columns, meta=None):
    try:
        cache_path = å¿«å–è³‡æ–™å¤¾ / f"{Path(file_path).stem}_å·²è™•ç†è³‡æ–™_v{CACHE_VERSION}.npz"
        if tensor is not None:
            np.savez_compressed(cache_path, tensor=tensor.cpu().numpy(), columns=np.array(columns), meta=meta)
            logger.info(f"å·²å¿«å–: {file_path} â†’ {cache_path}")
            conn = sqlite3.connect(å¿«å–è³‡æ–™å¤¾.parent / "SQLite" / "é è™•ç†ç´€éŒ„.db")
            c = conn.cursor()
            uuid_val = str(uuid.uuid4())
            market, timeframe = Path(file_path).stem.split("_")[:2]
            c.execute("UPDATE é è™•ç†ç´€éŒ„è¡¨ SET å¿«å–æª”å = ?, æ˜¯å¦æˆåŠŸ = ?, ç­†æ•¸ = ?, æ¬„ä½å®Œæ•´æ€§ = ?, ç‰ˆæœ¬è™Ÿ = ? WHERE ä¾†æºæª”æ¡ˆå = ?",
                      (str(cache_path), True, meta["rows"], True, CACHE_VERSION, Path(file_path).name))
            conn.commit()
            conn.close()
            return cache_path
        else:
            logger.error(f"ç„¡æ³•å„²å­˜å¿«å–ï¼Œtensor ç‚º None: {file_path}")
            asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç„¡æ³•å„²å­˜å¿«å–ï¼Œtensor ç‚º None: {file_path}", None, None, "è³‡æ–™é è™•ç†"))
            return None
    except Exception as e:
        logger.error(f"å¿«å–å„²å­˜å¤±æ•— [{file_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šå¿«å–å„²å­˜å¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šå¿«å–å„²å­˜å¤±æ•—: {e}", "å¿«å–å„²å­˜éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†", str(file_path)))
        return None

def check_system_resource():
    try:
        ram_ok = psutil.virtual_memory().used < MAX_RAM_USE
        cpu_ok = psutil.cpu_percent() < è³‡æºé–¾å€¼["CPUä½¿ç”¨ç‡"] * 100
        disk = psutil.disk_usage(str(å¿«å–è³‡æ–™å¤¾))
        disk_ok = (disk.free / disk.total) * 100 > è³‡æºé–¾å€¼["ç¡¬ç¢Ÿå‰©é¤˜æ¯”ä¾‹"]
        if torch.cuda.is_available():
            gpu_prop = torch.cuda.get_device_properties(è¨“ç·´è¨­å‚™)
            gpu_max = int(gpu_prop.total_memory * MAX_GPU_USE)
            gpu_ok = torch.cuda.memory_allocated(è¨“ç·´è¨­å‚™) < gpu_max
        else:
            gpu_ok = True
        return ram_ok and cpu_ok and disk_ok and gpu_ok
    except Exception as e:
        logger.error(f"è³‡æºæª¢æŸ¥å¤±æ•—: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šè³‡æºæª¢æŸ¥å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†"))
        return False

def generate_technical_indicators(df, periods=None):
    try:
        if periods is None:
            periods = {
                "SMA": è¨“ç·´åƒæ•¸["SMAé€±æœŸ"]["å€¼"] if è¨“ç·´åƒæ•¸["SMAé€±æœŸ"]["å•Ÿç”¨"] else 50,
                "HMA": è¨“ç·´åƒæ•¸["HMAé€±æœŸ"]["å€¼"] if è¨“ç·´åƒæ•¸["HMAé€±æœŸ"]["å•Ÿç”¨"] else 16,
                "ATR": è¨“ç·´åƒæ•¸["ATRé€±æœŸ"]["å€¼"] if è¨“ç·´åƒæ•¸["ATRé€±æœŸ"]["å•Ÿç”¨"] else 14,
                "VHF": è¨“ç·´åƒæ•¸["VHFé€±æœŸ"]["å€¼"] if è¨“ç·´åƒæ•¸["VHFé€±æœŸ"]["å•Ÿç”¨"] else 28,
                "Pivot": è¨“ç·´åƒæ•¸["Pivoté€±æœŸ"]["å€¼"] if è¨“ç·´åƒæ•¸["Pivoté€±æœŸ"]["å•Ÿç”¨"] else 5
            }
        custom_indicators = è¨“ç·´åƒæ•¸.get("custom_indicators", {})
        arr_close = cp.asarray(df['close'].values, dtype=cp.float32)
        arr_high = cp.asarray(df['high'].values, dtype=cp.float32)
        arr_low = cp.asarray(df['low'].values, dtype=cp.float32)
        if è¨“ç·´åƒæ•¸["SMAé€±æœŸ"]["å•Ÿç”¨"]:
            df['SMA50'] = try_gpu_indicator(calc_sma, arr_close, periods["SMA"])
        if è¨“ç·´åƒæ•¸["HMAé€±æœŸ"]["å•Ÿç”¨"]:
            df['HMA_16'] = try_gpu_indicator(calc_hma, arr_close, periods["HMA"])
        if è¨“ç·´åƒæ•¸["ATRé€±æœŸ"]["å•Ÿç”¨"]:
            df['ATR_14'] = try_gpu_indicator(calc_atr, arr_high, arr_low, arr_close, periods["ATR"])
        if è¨“ç·´åƒæ•¸["VHFé€±æœŸ"]["å•Ÿç”¨"]:
            df['VHF_28'] = try_gpu_indicator(calc_vhf, arr_close, periods["VHF"])
        if è¨“ç·´åƒæ•¸["Pivoté€±æœŸ"]["å•Ÿç”¨"]:
            df['PivotHigh'] = try_gpu_indicator(calc_pivot_high, arr_high, periods["Pivot"])
            df['PivotLow'] = try_gpu_indicator(calc_pivot_low, arr_low, periods["Pivot"])
        for name, config in custom_indicators.items():
            if config.get("enabled", False):
                df[name] = try_gpu_indicator(eval(config["function"]), arr_close, config.get("window", 14))
        conn = sqlite3.connect(å¿«å–è³‡æ–™å¤¾.parent / "SQLite" / "é è™•ç†ç´€éŒ„.db")
        c = conn.cursor()
        uuid_val = str(uuid.uuid4())
        market, timeframe = Path(file_path).stem.split("_")[:2]
        c.execute("INSERT INTO é è™•ç†ç´€éŒ„è¡¨ (UUID, å¸‚å ´, é€±æœŸ, æŠ€è¡“æŒ‡æ¨™) VALUES (?, ?, ?, ?)",
                  (uuid_val, market, timeframe, str({**periods, **{k: v.get("window", 14) for k, v in custom_indicators.items()}})))
        conn.commit()
        conn.close()
        return df
    except Exception as e:
        logger.error(f"æŠ€è¡“æŒ‡æ¨™é‹ç®—å¤±æ•—: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæŠ€è¡“æŒ‡æ¨™é‹ç®—å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæŠ€è¡“æŒ‡æ¨™é‹ç®—å¤±æ•—: {e}", "æŒ‡æ¨™é‹ç®—éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return df

async def adjust_chunk_size(current_chunk_size):
    try:
        memory = psutil.virtual_memory()
        ram_percent = memory.percent
        disk = psutil.disk_usage(str(å¿«å–è³‡æ–™å¤¾))
        disk_free_percent = (disk.free / disk.total) * 100
        cpu_ok = psutil.cpu_percent() < è³‡æºé–¾å€¼["CPUä½¿ç”¨ç‡"] * 100
        gpu_ok = True
        if torch.cuda.is_available():
            gpu_prop = torch.cuda.get_device_properties(è¨“ç·´è¨­å‚™)
            gpu_max = int(gpu_prop.total_memory * MAX_GPU_USE)
            gpu_ok = torch.cuda.memory_allocated(è¨“ç·´è¨­å‚™) < gpu_max
        if ram_percent > è³‡æºé–¾å€¼["RAMä½¿ç”¨ç‡"] or disk_free_percent < è³‡æºé–¾å€¼["ç¡¬ç¢Ÿå‰©é¤˜æ¯”ä¾‹"] or not cpu_ok or not gpu_ok:
            new_size = max(MIN_CHUNK_SIZE, current_chunk_size // 2)
            logger.info(f"è³‡æºè¶…é™: RAM={ram_percent}%, ç¡¬ç¢Ÿå‰©é¤˜={disk_free_percent}%, CPU={'æ­£å¸¸' if cpu_ok else 'è¶…è¼‰'}, GPU={'æ­£å¸¸' if gpu_ok else 'è¶…è¼‰'}ï¼Œèª¿æ•´ CHUNK_SIZE è‡³ {new_size}")
            await ç™¼é€é€šçŸ¥(f"ã€é€šçŸ¥ã€‘è³‡æºè¶…é™: RAM={ram_percent}%, ç¡¬ç¢Ÿå‰©é¤˜={disk_free_percent}%, CPU={'æ­£å¸¸' if cpu_ok else 'è¶…è¼‰'}, GPU={'æ­£å¸¸' if gpu_ok else 'è¶…è¼‰'}ï¼Œèª¿æ•´ CHUNK_SIZE è‡³ {new_size}", None, None, "è³‡æ–™é è™•ç†")
            return new_size
        return current_chunk_size
    except Exception as e:
        logger.error(f"CHUNK_SIZE èª¿æ•´å¤±æ•—: {e}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šCHUNK_SIZE èª¿æ•´å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†")
        return current_chunk_size

def split_datasets(df, file_path):
    try:
        train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1
        train_size = int(len(df) * train_ratio)
        val_size = int(len(df) * val_ratio)
        train_df = df.iloc[:train_size]
        val_df = df.iloc[train_size:train_size + val_size]
        test_df = df.iloc[train_size + val_size:]
        market, timeframe = Path(file_path).stem.split("_")[:2]
        train_path = å¿«å–è³‡æ–™å¤¾ / f"{market}_{timeframe}_train_v{CACHE_VERSION}.npz"
        val_path = å¿«å–è³‡æ–™å¤¾ / f"{market}_{timeframe}_val_v{CACHE_VERSION}.npz"
        test_path = å¿«å–è³‡æ–™å¤¾ / f"{market}_{timeframe}_test_v{CACHE_VERSION}.npz"
        for subset, path in [(train_df, train_path), (val_df, val_path), (test_df, test_path)]:
            tensor = torch_tensorize(subset, device='cpu')
            if tensor is not None:
                np.savez_compressed(path, tensor=tensor.cpu().numpy(), columns=np.array(subset.columns))
        conn = sqlite3.connect(å¿«å–è³‡æ–™å¤¾.parent / "SQLite" / "é è™•ç†ç´€éŒ„.db")
        c = conn.cursor()
        uuid_val = str(uuid.uuid4())
        c.execute("INSERT INTO é è™•ç†ç´€éŒ„è¡¨ (UUID, å¸‚å ´, é€±æœŸ, è¨“ç·´é›†ç­†æ•¸, é©—è­‰é›†ç­†æ•¸, æ¸¬è©¦é›†ç­†æ•¸, ç‰ˆæœ¬è™Ÿ) VALUES (?, ?, ?, ?, ?, ?, ?)",
                  (uuid_val, market, timeframe, len(train_df), len(val_df), len(test_df), CACHE_VERSION))
        conn.commit()
        conn.close()
        return {"train": train_path, "val": val_path, "test": test_path}
    except Exception as e:
        logger.error(f"æ•¸æ“šé›†åˆ†å‰²å¤±æ•— [{file_path}]: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ•¸æ“šé›†åˆ†å‰²å¤±æ•— [{file_path}]: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæ•¸æ“šé›†åˆ†å‰²å¤±æ•—: {e}", "æ•¸æ“šé›†åˆ†å‰²éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†", str(file_path)))
        return None

async def process_single_file(file_path, log_writer, periods=None, retry_count=0):
    try:
        start_time = time.time()
        file_path = Path(file_path)
        if not await æª¢æŸ¥æª”æ¡ˆè·¯å¾‘(file_path):
            logger.error(f"ç„¡æ•ˆæª”æ¡ˆè·¯å¾‘: {file_path}")
            await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç„¡æ•ˆæª”æ¡ˆè·¯å¾‘: {file_path}", None, None, "è³‡æ–™é è™•ç†")
            return None
        cache_path = å¿«å–è³‡æ–™å¤¾ / f"{file_path.stem}_å·²è™•ç†è³‡æ–™_v{CACHE_VERSION}.npz"
        if cache_path.exists() and check_md5_consistency(file_path, cache_path):
            logger.info(f"MD5ä¸€è‡´ï¼Œè·³éè™•ç†: {file_path}")
            await ç™¼é€é€šçŸ¥(f"ã€é€šçŸ¥ã€‘MD5ä¸€è‡´ï¼Œè·³éè™•ç†: {file_path}", None, None, "è³‡æ–™é è™•ç†")
            return cache_path
        required_cols = ['open', 'high', 'low', 'close', 'timestamp'] + æŠ€è¡“æŒ‡æ¨™åå–®
        if file_path.suffix.lower() == ".zst":
            df_source = load_zst_csv(file_path)
        elif file_path.suffix.lower() == ".gz":
            with gzip.open(file_path, "rt", encoding="utf-8-sig") as f:
                df_source = pd.read_csv(f)
        else:
            encoding = detect_encoding(file_path)
            df_source = pd.read_csv(file_path, encoding=encoding)
        if df_source is None:
            return None
        available_cols = df_source.columns.str.lower().str.strip()
        time_col = guess_time_column(available_cols)
        if not time_col:
            logger.error(f"ç„¡å¯ç”¨æ™‚é–“æ¬„ä½: {df_source.columns.tolist()}")
            await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç„¡å¯ç”¨æ™‚é–“æ¬„ä½: {df_source.columns.tolist()}", None, None, "è³‡æ–™é è™•ç†")
            return None
        df_source = parse_datetime_column(df_source, time_col)
        market, timeframe = Path(file_path).stem.split("_")[:2]
        if not check_time_continuity(df_source, timeframe, file_path):
            return None
        total_rows = len(df_source)
        chunk_thresh = 500000
        all_chunks = []
        current_chunk_size = CHUNK_SIZE
        checkpoint = 0
        if total_rows > chunk_thresh:
            chunker = pd.read_csv(file_path, encoding=detect_encoding(file_path), chunksize=current_chunk_size)
            last_overlap = None
            for chunk_idx, chunk in enumerate(chunker):
                if not check_system_resource():
                    current_chunk_size = await adjust_chunk_size(current_chunk_size)
                    chunker = pd.read_csv(file_path, encoding=detect_encoding(file_path), chunksize=current_chunk_size)
                    torch.cuda.empty_cache()
                    await asyncio.sleep(0.005)
                if last_overlap is not None:
                    chunk = pd.concat([last_overlap, chunk], ignore_index=True)
                chunk = await process_chunk(chunk, required_cols, periods)
                if chunk is None:
                    continue
                last_overlap = chunk.iloc[-OVERLAP:].copy()
                all_chunks.append(chunk.iloc[OVERLAP:].reset_index(drop=True) if len(all_chunks) > 0 else chunk.reset_index(drop=True))
                checkpoint = chunk_idx + 1
                del chunk
                torch.cuda.empty_cache()
            if all_chunks:
                df_final = pd.concat(all_chunks, ignore_index=True)
            else:
                logger.error(f"ç„¡æœ‰æ•ˆåˆ†å¡Šæ•¸æ“š: {file_path}, æ¢å¾©é»: {checkpoint}")
                await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç„¡æœ‰æ•ˆåˆ†å¡Šæ•¸æ“š: {file_path}, æ¢å¾©é»: {checkpoint}", None, None, "è³‡æ–™é è™•ç†")
                return None
        else:
            df_final = await process_chunk(df_source, required_cols, periods)
            if df_final is None:
                return None
        if not check_and_log_anomalies(df_final, log_writer, file_path):
            return None
        dataset_paths = split_datasets(df_final, file_path)
        if dataset_paths is None:
            return None
        tensor = torch_tensorize(df_final, device='cpu')
        meta = {"cols": df_final.columns.tolist(), "rows": len(df_final), "train_path": str(dataset_paths["train"]), "val_path": str(dataset_paths["val"]), "test_path": str(dataset_paths["test"])}
        path = save_cache(file_path, tensor, df_final.columns, meta=meta)
        elapsed_time = time.time() - start_time
        if path:
            await ç™¼é€é€šçŸ¥(
                f"ã€è³‡æ–™é è™•ç†å®Œæˆã€‘\nUUID: {uuid_val}\nå¸‚å ´: {market}\né€±æœŸ: {timeframe}\nç­†æ•¸: {meta['rows']}\nGPU: {torch.cuda.is_available()}\nå¿«å–: {path}\nè™•ç†æ™‚é–“: {elapsed_time:.2f}ç§’\næ™‚é–“: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                market, timeframe, "è³‡æ–™é è™•ç†"
            )
        return dataset_paths
    except Exception as e:
        if retry_count < 5:
            logger.warning(f"æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œé‡è©¦ {retry_count + 1}/5: {e}, æ¢å¾©é»: {checkpoint}")
            await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæª”æ¡ˆè™•ç†å¤±æ•— é‡è©¦{retry_count + 1}/5: {e}, æ¢å¾©é»: {checkpoint}", "æª”æ¡ˆè™•ç†éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†", str(file_path))
            await asyncio.sleep(5)
            return await process_single_file(file_path, log_writer, periods, retry_count + 1)
        logger.error(f"æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ [{file_path}]: {e}, æ¢å¾©é»: {checkpoint}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæª”æ¡ˆè™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ [{file_path}]: {e}, æ¢å¾©é»: {checkpoint}", None, None, "è³‡æ–™é è™•ç†")
        await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæª”æ¡ˆè™•ç†å¤±æ•—: {e}, æ¢å¾©é»: {checkpoint}", "æª”æ¡ˆè™•ç†éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†", str(file_path))
        return None

async def process_chunk(chunk, required_cols, periods=None, retry_count=0):
    try:
        col_map = smart_column_map(chunk.columns)
        chunk = chunk.rename(columns=col_map)
        chunk = ensure_all_columns(chunk, ['open', 'high', 'low', 'close', 'timestamp'])
        chunk = generate_technical_indicators(chunk, periods)
        chunk = ensure_all_columns(chunk, required_cols)
        if 'timestamp' in chunk.columns and not np.issubdtype(chunk['timestamp'].dtype, np.datetime64):
            chunk = parse_datetime_column(chunk, 'timestamp')
        chunk = chunk[required_cols]
        return chunk
    except Exception as e:
        if retry_count < 5:
            logger.warning(f"åˆ†å¡Šè™•ç†å¤±æ•—ï¼Œé‡è©¦ {retry_count + 1}/5: {e}")
            await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šåˆ†å¡Šè™•ç†å¤±æ•— é‡è©¦{retry_count + 1}/5: {e}", "åˆ†å¡Šè™•ç†éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†")
            await asyncio.sleep(5)
            return await process_chunk(chunk, required_cols, periods, retry_count + 1)
        logger.error(f"åˆ†å¡Šè™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ: {e}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šåˆ†å¡Šè™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ: {e}", None, None, "è³‡æ–™é è™•ç†")
        await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šåˆ†å¡Šè™•ç†å¤±æ•—: {e}", "åˆ†å¡Šè™•ç†éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†")
        return None

async def load_and_preprocess_data(market, timeframe, periods=None, retry_count=0):
    try:
        start_time = time.time()
        file_path = å¿«å–è³‡æ–™å¤¾.parent / f"è¨“ç·´è³‡æ–™/{market}_{timeframe}.csv"
        if not file_path.exists():
            file_path = å¿«å–è³‡æ–™å¤¾.parent / f"è¨“ç·´è³‡æ–™/{market}_{timeframe}.zst"
        if not file_path.exists():
            file_path = å¿«å–è³‡æ–™å¤¾.parent / f"è¨“ç·´è³‡æ–™/{market}_{timeframe}.gz"
        if not await æª¢æŸ¥æª”æ¡ˆè·¯å¾‘(file_path):
            logger.error(f"[{market}_{timeframe}] ç„¡æ•ˆæª”æ¡ˆè·¯å¾‘: {file_path}")
            await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šç„¡æ•ˆæª”æ¡ˆè·¯å¾‘: {file_path}", market, timeframe, "è³‡æ–™é è™•ç†")
            return None
        log_path = log_dir / f"è³‡æ–™é è™•ç†æ—¥èªŒ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        with open(log_path, 'w', encoding='utf-8') as log_writer:
            dataset_paths = await process_single_file(file_path, log_writer, periods)
        if dataset_paths:
            elapsed_time = time.time() - start_time
            logger.info(f"[{market}_{timeframe}] æ•¸æ“šé è™•ç†å®Œæˆï¼Œè€—æ™‚: {elapsed_time:.2f}ç§’")
            return dataset_paths
        return None
    except Exception as e:
        if retry_count < 5:
            logger.warning(f"[{market}_{timeframe}] æ•¸æ“šé è™•ç†å¤±æ•—ï¼Œé‡è©¦ {retry_count + 1}/5: {e}")
            await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæ•¸æ“šé è™•ç†å¤±æ•— é‡è©¦{retry_count + 1}/5: {e}", "æ•¸æ“šé è™•ç†éŒ¯èª¤", market, timeframe, "è³‡æ–™é è™•ç†")
            await asyncio.sleep(5)
            return await load_and_preprocess_data(market, timeframe, periods, retry_count + 1)
        logger.error(f"[{market}_{timeframe}] æ•¸æ“šé è™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ: {e}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ•¸æ“šé è™•ç†å¤±æ•—ï¼Œé‡è©¦ 5 æ¬¡ç„¡æ•ˆ: {e}", market, timeframe, "è³‡æ–™é è™•ç†")
        await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæ•¸æ“šé è™•ç†å¤±æ•—: {e}", "æ•¸æ“šé è™•ç†éŒ¯èª¤", market, timeframe, "è³‡æ–™é è™•ç†")
        return None

async def main():
    try:
        listener = start_log_listener()
        await cleanup_expired_cache()
        zst_files = list(å¿«å–è³‡æ–™å¤¾.parent.glob("è¨“ç·´è³‡æ–™/*.zst")) + list(å¿«å–è³‡æ–™å¤¾.parent.glob("è¨“ç·´è³‡æ–™/*.gz")) + list(å¿«å–è³‡æ–™å¤¾.parent.glob("è¨“ç·´è³‡æ–™/*.csv"))
        total_files = len(zst_files)
        if total_files == 0:
            logger.error("æœªæ‰¾åˆ°ä»»ä½• .zstã€.gz æˆ– .csv æª”æ¡ˆ")
            await ç™¼é€éŒ¯èª¤è¨Šæ¯("éŒ¯èª¤ç¢¼E502ï¼šæœªæ‰¾åˆ°ä»»ä½• .zstã€.gz æˆ– .csv æª”æ¡ˆ", None, None, "è³‡æ–™é è™•ç†")
            log_queue.put(None)
            listener.terminate()
            return
        now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_path = log_dir / f"è³‡æ–™é è™•ç†æ—¥èªŒ_{now}.log"
        current_proc = MAX_PROC
        with open(log_path, 'w', encoding='utf-8') as log:
            log.write(f"ğŸ“ å…±ç™¼ç¾æª”æ¡ˆæ•¸é‡ï¼š{total_files}\n")
            with mp.Pool(processes=current_proc) as pool:
                for idx, file_path in enumerate(zst_files, 1):
                    if not check_system_resource():
                        _, current_proc = await ç›£æ§ç¡¬é«”ç‹€æ…‹ä¸¦é™ç´š(CHUNK_SIZE, current_proc, priority=1)
                        pool.close()
                        pool.join()
                        pool = mp.Pool(processes=current_proc)
                    log.write(f"[{idx}/{total_files}] ğŸ•’ è™•ç†ä¸­ï¼š{file_path.name}ï¼Œä½¿ç”¨é€²ç¨‹æ•¸ï¼š{current_proc}\n")
                    pool.apply_async(process_single_file, args=(file_path, log))
                pool.close()
                pool.join()
            log.write(f"âœ… é è™•ç†å®Œæˆï¼Œè©³ç´°è¨˜éŒ„è¦‹ï¼š{log_path}\n")
            await ç™¼é€é€šçŸ¥(f"ã€é€šçŸ¥ã€‘é è™•ç†å®Œæˆï¼Œè©³ç´°è¨˜éŒ„: {log_path}ï¼Œç¸½é€²ç¨‹æ•¸: {current_proc}", None, None, "è³‡æ–™é è™•ç†")
        log_queue.put(None)
        listener.terminate()
    except Exception as e:
        logger.error(f"ä¸»æµç¨‹å¤±æ•—: {e}")
        await ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šè³‡æ–™é è™•ç†ä¸»æµç¨‹å¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†")
        await éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šä¸»æµç¨‹å¤±æ•—: {e}", "ä¸»æµç¨‹éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†")
        log_queue.put(None)
        listener.terminate()

def å–å¾—Kfoldè³‡æ–™é›†(dataset, K=5, shuffle=True, random_state=42, batch_size=128):
    try:
        kf = KFold(n_splits=K, shuffle=shuffle, random_state=random_state)
        fold_data = []
        for train_idx, val_idx in kf.split(range(len(dataset))):
            train_subset = Subset(dataset, train_idx)
            val_subset = Subset(dataset, val_idx)
            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
            fold_data.append((train_loader, val_loader))
        return fold_data
    except Exception as e:
        logger.error(f"KFold è³‡æ–™é›†ç”Ÿæˆå¤±æ•—: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šKFold è³‡æ–™é›†ç”Ÿæˆå¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šKFold è³‡æ–™é›†ç”Ÿæˆå¤±æ•—: {e}", "KFold éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return []

def æ»‘å‹•çª—å£è³‡æ–™ç”Ÿæˆ(X, y, window_size=60, step_size=1):
    try:
        Xs, ys = [], []
        N = len(X)
        for i in range(0, N - window_size + 1, step_size):
            Xs.append(X[i:i+window_size])
            ys.append(y[i+window_size-1])
        return np.array(Xs), np.array(ys)
    except Exception as e:
        logger.error(f"æ»‘å‹•çª—å£è³‡æ–™ç”Ÿæˆå¤±æ•—: {e}")
        asyncio.run(ç™¼é€éŒ¯èª¤è¨Šæ¯(f"éŒ¯èª¤ç¢¼E502ï¼šæ»‘å‹•çª—å£è³‡æ–™ç”Ÿæˆå¤±æ•—: {e}", None, None, "è³‡æ–™é è™•ç†"))
        asyncio.run(éŒ¯èª¤è¨˜éŒ„èˆ‡è‡ªå‹•ä¿®å¾©(f"éŒ¯èª¤ç¢¼E502ï¼šæ»‘å‹•çª—å£è³‡æ–™ç”Ÿæˆå¤±æ•—: {e}", "æ»‘å‹•çª—å£éŒ¯èª¤", None, None, "è³‡æ–™é è™•ç†"))
        return np.array([]), np.array([])

if __name__ == "__main__":
    asyncio.run(main())